Question:
---------
Write a R-Program to represent multiclass classification using Scikit learn with representing the problem using k-nearest neighbours

Solution:
--------
* K-Nearest Neighnours is a supervised   machine learning.
* This takes a set of input objects and   input values.
* Then trains on that data to learn how to   map the inputs to the desired output so it   predicts       unseen data.

For Example:
- If k=5 and 3 of points are green and 2 are   red then the data point in question would   be labeled    green,since green is the majority.  

Now considering the Diabetes dataset,the first step is to "Read in the data" which is used as input.

import pandas as pd
#read in the data using pandas
df = pd.read_csv(‘data/diabetes_data.csv’)
#check data has been read in properly
df.head()

Question:
---------
Write a R-Program to represent multiclass classification using Scikit learn with representing the problem using k-nearest neighbours

Solution:
--------
* K-Nearest Neighnours is a supervised   machine learning.
* This takes a set of input objects and   input values.
* Then trains on that data to learn how to   map the inputs to the desired output so it   predicts       unseen data.

For Example:
- If k=5 and 3 of points are green and 2 are   red then the data point in question would   be labeled    green,since green is the majority.  

Now considering the Diabetes dataset,the first step is to "Read in the data" which is used as input.

  import pandas as pd
  #read in the data using pandas
  df = pd.read_csv(‘data/diabetes_data.csv’)
  #check data has been read in properly
  df.head()

                 SNO: preg: glucose: diastolic: triceps: insulin: bmi:  dpf:  age: diabetes:
                 _________________________________________________________________________
                 0    6     148      72         35        0      33.6   0.627  50    1                       
                 1    1     85       66         29        0      26.6   0.351  31    0                      
                 2    8     183      64         0         0      23.3   0.672  32    1
                          
                 3    1     89       66         23        94     28.1   0.167  21    0
                         
                 4    0     137      40         35        168    43.1   2.288  33    1                            
  #check number of rows and columns in dataset
  df.shape     -->(768,9)

  #create a dataframe with all training data except the target column
  X = df.drop(columns=[‘diabetes’])
  #check that the target variable has been removed
  X.head()
                 
              SNO: preg: glucose: diastolic: triceps:  insulin:  bmi:    dpf:  age:
              ____________________________________________________________________
                 0    6     148      72         35        0      33.6   0.627  50                          
                 1    1     85       66         29        0      26.6   0.351  31                          
                 2    8     183      64         0         0      23.3   0.672  32    
                          
                 3    1     89       66         23        94     28.1   0.167  21    
                         
                 4    0     137      40         35        168    43.1   2.288  33

We will insert the ‘diabetes’ column of our dataset into our target variable (y)

  #separate target values
  y = df[‘diabetes’].values
  #view target values
  y[0:5]   --> which gives us array([1,0,1,0,1])

Split the dataset into train and test data:
------------------------------------------

   from sklearn.model_selection import train_test_split
   #split dataset into train and test data
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,       stratify=y)
   
‘train_test_split’ takes in 5 parameters. The first two parameters are the input and target data we split up earlier. Next, we will set ‘test_size’ to 0.2. This means that 20% of all the data will be used for testing, which leaves 80% of the data as training data for the model to learn from. Setting ‘random_state’ to 1 ensures that we get the same split each time so we can reproduce our results.
   
Building and training the model:
-------------------------------
  from sklearn.neighbors import KNeighborsClassifier
  # Create KNN classifier
  knn = KNeighborsClassifier(n_neighbors = 3)
  # Fit the classifier to the data
  knn.fit(X_train,y_train)

First, we will create a new k-NN classifier and set ‘n_neighbors’ to 3. To recap, this means that if at least 2 out of the 3 nearest points to an new data point are patients without diabetes, then the new data point will be labeled as ‘no diabetes’, and vice versa. In other words, a new data point is labeled with by majority from the 3 nearest points.

Testing the model:
------------------
Once the model is trained, we can use the ‘predict’ function on our model to make predictions on our test data. As seen when inspecting ‘y’ earlier, 0 indicates that the patient does not have diabetes and 1 indicates that the patient does have diabetes. To save space, we will only show print the first 5 predictions of our test set.

  #show first 5 model predictions on the test data
  knn.predict(X_test)[0:5]  --> which gives us array([0,0,0,1])


To see how our accurate our model is on the full test set. To do this, we will use the ‘score’ function and pass in our test input and target data to see how well our model predictions match up to the actual results.

  #check accuracy of our model on the test data
  knn.score(X_test, y_test) --->which gives us a value of 0.66883116883116878

* Our model has an accuracy of approximately 66.88%

k-Fold Cross-Validation:
-----------------------
In order to train and test our model using cross-validation, we will use the ‘cross_val_score’ function with a cross-validation value of 5. 

To find the average of the 5 scores, we will use numpy’s mean function, passing in ‘cv_score’. Numpy is a useful math library in Python.
  
  from sklearn.model_selection import cross_val_score
  import numpy as np
  #create a new KNN model
  knn_cv = KNeighborsClassifier(n_neighbors=3)
  #train model with cv of 5 
  cv_scores = cross_val_score(knn_cv, X, y, cv=5)
  #print each cv score (accuracy) and average them
  print(cv_scores)
  print(‘cv_scores mean:{}’.format(np.mean(cv_scores)))
 
-->which gives us [0.68181818 0.69480519 0.75324675 0.68627451 cv_scores_mean:0.7135557253204311

Hypertuning model parameters using GridSearchCV:
------------------------------------------------
* Hypertuning parameters is when you go through a process to find the optimal parameters for your  model to improve accuracy. In our case, we will use GridSearchCV to find the optimal value for  ‘n_neighbors’.

* Our new model using grid search will take in a new k-NN classifier, our param_grid and a cross-    validation value of 5 in order to find the optimal value for ‘n_neighbors’.

  from sklearn.model_selection import GridSearchCV
  #create new a knn model
  knn2 = KNeighborsClassifier()
  #create a dictionary of all values we want to test for n_neighbors
  param_grid = {‘n_neighbors’: np.arange(1, 25)}
  #use gridsearch to test all values for n_neighbors
  knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
  #fit model to data
  knn_gscv.fit(X, y)

  #check top performing n_neighbors value
  knn_gscv.best_params_ 

-->which gives us {'n_neighbours' : 14}

   #check mean score for the top performing value of n_neighbors
   knn_gscv.best_score_

-->which gives us a value of 0.7578125

* By using grid search to find the optimal parameter for our model, we have improved our model   accuracy by over 4%!
